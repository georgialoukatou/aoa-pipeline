---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)

# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng}
#eng_wb_data <- load_wb_data("English (American)")
#eng_wb_data
```

Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}

target_langs <- c("French (French)", "French (Quebecois)", "Spanish (Mexican)", "Spanish (European)",  "German", "Portuguese (European)", "Hungarian", "Swedish", "Italian", "Norwegian", "Dutch", "Turkish", "Danish")

target_langs <- c("Danish")

wb_data <- load_wb_data(target_langs)
aoas <- fit_aoas(wb_data)

childes_lang = "dan"
l="dan"

```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
eng_wg <- create_inst_data("English (American)", "WG")
eng_ws <- create_inst_data("English (American)", "WS")
eng_wg_summary <- collapse_inst_data(eng_wg)
eng_ws_summary <- collapse_inst_data(eng_ws)
eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- extract_uni_lemmas(wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)

valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)

concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

## CHILDES

Loading cached CHILDES metrics for English:
```{r load_childes_eng, eval=FALSE}
eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Loading cached CHILDES data for multiple languages:
```{r load_childes_xling, eval=FALSE}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |>
  filter(!is.na(uni_lemma))
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes}

metric_funs <- list(compute_count, compute_mlu, compute_positions, compute_length_char, compute_length_phon,  compute_n_sfx_cat, compute_n_type, compute_verb_frame ) 

corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")


dan_childes <- get_token_metrics("Danish", metric_funs, corpus_args)
dan_unilemmas <- get_uni_lemma_metrics("Danish", build_uni_lemma_map(uni_lemmas|> filter(language =="Danish")))

```

Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=FALSE}
walk(target_langs, get_token_metrics, metric_funs, corpus_args)
walk(target_langs, load_childes_metrics, uni_lemmas)
```

Get phonology via eSpeak for tokens that didn't get it from CHILDES:
```{r}
#phonemes <- uni_lemmas |> map_phonemes()
#childes_metrics <- childes_metrics |>
#  left_join(phonemes, by = c("language", "uni_lemma")) |>
#  mutate(length_char = coalesce(length_char, num_chars),
#         length_phon = coalesce(length_phon, num_phons)) |>
#  select(-c(num_chars, num_phons))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}

childes_metrics <- dan_unilemmas 
childes_metrics <- childes_metrics |> transform_counts() |> residualize_freqs() 
childes_metrics <- map_df(target_langs, residualize_morph, childes_metrics)
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}

predictor_data_list <- list(babiness, concreteness, childes_metrics)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 

```

## Setting predictors

```{r set_predictors}
predictor_sources <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type",  "n_cat", "n_affix"), 
  c("per_frame"),
  c("mlu"))


predictors<-unlist(predictor_sources)

```


## Preparing data for regression

```{r prep_data}

ref_cat ="nouns"

predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas, ref_cat) ## Set lexical categories
predictor_data_lexcat  <- predictor_data_lexcat  |> unnest(options)


#Imputed and scaling
predictor_data_imputed_scaled <- predictor_data_lexcat |> #
  do_full_imputation(predictor_sources1, max_steps = 20) 

```
 
## Merge with AOAs

```{r merge_aoa}

aoa_predictor_data <- predictor_data_imputed_scaled |> left_join(aoas) |>## merge with AOAs
  filter(!(aoa>36))
 
```

## Run model 

```{r}

l1 = c("Spanish (Mexican)", "French (French)", "French (Quebecois)", "Spanish (European)", "German", "Swedish", "Portuguese (European)", "Hungarian")

sinotibetan_languages <- c("Mandarin (Taiwanese)", "Mandarin (Beijing)")


run_main_model<-function(lang, aoa_predictor_data, predictors, uni_lemmas, ref_cat, lexcat_interactions = TRUE){

   if (lang %in% l1){
    print(predictors)
  } else {
    predictors = predictors[predictors!= "n_affix"]
    print(predictors)
  }
  if (lang %in% sinotibetan_languages){
    predictors = predictors[predictors!= "length_char"]
  }  

  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts

  m<-fit_models(predictors, aoa_predictor_data_lexcat, lexcat_interactions) |> 
    mutate(language=lang)
  
  return(m)
}

#saveRDS(aoa_predictor_data, "data/aoa_predictor_data.rds")


aoa_models <- map_df(target_langs, run_main_model, aoa_predictor_data, predictors, uni_lemmas, ref_cat)

```

## Add per_frame
```{r}

predictor_sources_verb <- list(
  c("freq"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type",  "n_cat", "n_affix"), #
  c("per_frame"),
  c("mlu"))

predictors_verb<-unlist(predictor_sources_verb)

aoa_models_verb <- map_df(target_langs, run_main_model, aoa_predictor_data|>filter(lexical_category =="predicates"), predictors_verb, uni_lemmas, ref_cat, lexcat_interactions = FALSE)


aoa_models_verb <- aoa_models_verb |> unnest(coefs) |>filter(term =="per_frame") 

to_add <- aoa_models |>
  select(language, measure, group_data, stats, vifs)

aoa_models <- aoa_models |> 
  unnest(coefs) |>
  rbind(aoa_models_verb) |>
  select(-c(group_data,stats, model, vifs )) |>
  nest(coefs =c(term, estimate, std.error, statistic, p.value)) |>
  left_join(to_add)
  
```

## Model outputs

Coefficients:
```{r}
aoa_models |> select(language, measure, coefs) |> unnest(coefs) 

```

Summary stats:
```{r}
aoa_models |> select(language, measure, stats) |> unnest(stats)

```

Variance inflation factors:
```{r, eval = FALSE}
aoa_models |> select(language, measure, vifs) |> unnest(vifs)

```


## Cross-validation

```{r cross_validate}

run_cross_validation_model<-function(lang, meas, aoa_predictor_data, predictors, uni_lemmas, ref_cat){
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang, measure==meas)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts

   if (lang %in% l1){
    print(predictors)
  } else {
    predictors = predictors[predictors!= "n_affix"]
    print(predictors)
  }
    
  if (lang %in% sinotibetan_languages){
    predictors = predictors[predictors!= "length_char"]
  }  

loo_df <- aoa_predictor_data_lexcat |>
  group_nest(language, measure) |>
  mutate(language=lang,
         measure=meas,
    loo_models = ifelse(language %in% sinotibetan_languages, map(data, fit_cv_models, list(make_predictor_formula(predictors))), map(data, fit_cv_models, list(make_predictor_formula(predictors)))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))  
    
return(loo_df)
}

# for simple cv result extraction
xval_und_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="understands"))$language),run_cross_validation_model,"understands", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
xval_prod_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="produces"))$language),run_cross_validation_model,"produces", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 

loo_df <- bind_rows(xval_und_model, xval_prod_model)

```


```{r cross_validate_results}

cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)

loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)


cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = ~ make.names(., unique = TRUE)) |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)

man_t_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "Spanish (Mexican)", measure == "understands") |>
  arrange(desc(abs_dev)) |>
  head(50)

```


```{r coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)


aoa_coefs <- aoa_models |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category[0-9]"),
                                    as.character(NA)),
         lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         term = fct_recode(term, !!!lexcats),
         term = factor(term), #, levels = c(predictors, names(lexcats))
         language = factor(language, levels = target_langs))

```


```{r savedata, eval = FALSE }
saveRDS(predictor_data_lexcat,"data/plots/predictor_data_lexcat.rds")
saveRDS(aoa_predictor_data,"data/plots/aoa_predictor_data.rds" )

saveRDS(aoa_coefs, "data/plots/aoa_coefs.rds" )


saveRDS(cv_results, "data/plots/cv_results.rds" )
saveRDS(cv_results_pos, "data/plots/cv_results_pos.rds" )
saveRDS(cv_results_cat, "data/plots/cv_results_cat.rds" )
saveRDS(cv_results_lex, "data/plots/cv_results_lex.rds" )
saveRDS(eng_across_lang_lex_desc, "data/plots/eng_across_lang_lex_desc.rds" )


worst_predicted_unilemmas<- function(lang, meas, loo_preds){ loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == lang, measure == meas) |>
  arrange(desc(abs_dev)) |>
  head(50)
}

worst_uni_produce<-map_df(target_langs, worst_predicted_unilemmas, "produces", loo_preds) %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))

worst_uni_understand<-map_df(target_langs, worst_predicted_unilemmas, "understands", loo_preds) %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))

worst_uni_understand1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "understands") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))

worst_uni_produce1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "produces") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))


saveRDS(worst_uni_produce, "data/plots/worst_uni_produce.rds" )
saveRDS(worst_uni_understand, "data/plots/worst_uni_understand.rds" )


```
