---
title: "AoA prediction template"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(dplyr.summarise.inform = FALSE)

# load libraries
library(tidyverse)
library(modelr)
library(glue)
library(wordbankr)
library(childesr)

# load functions
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)
```

This template provides the structure for how to fit age of acquisition (AoA) prediction models, using data and scripts in this repository.

The general steps are:
- loading the provided CDI data and predictor data
- adding your predictor(s) to the provided predictor data
- using the functions in `scripts/prep_data.R` to prepare the data for modeling
- using the functions in `scripts/fit_models.R` to fit models and extract information from them


# Load Wordbank data

Loading cached Wordbank data for English:
```{r load_wordbank_eng}
#eng_wb_data <- load_wb_data("English (American)")
#eng_wb_data
```

Loading Wordbank data for multiple languages (cached or not):
```{r load_wordbank_xling}
target_langs <- c("English (American)", "Norwegian","Russian", 
                  "Turkish", "Italian", "Swedish",  
                  "French (Quebecois)", "Danish", "Croatian", 
                  "French (French)","German", 
                  "English (British)", "Spanish (European)", "Spanish (Mexican)",
                  "Portuguese (European)", "Mandarin (Beijing)", "English (Australian)")

#English American ok Spanish (Mexican) ok French French ok

target_langs <- c("Italian", "French (French)", "Spanish (Mexican)", "English (American)")

#Mandarin taiwanese:  problematic

sinotibetan_languages <- c("Mandarin (Taiwanese)", "Mandarin (Beijing)")

wb_data <- load_wb_data(target_langs)
aoas <- fit_aoas(wb_data)


```

Creating saved Wordbank data for a language, for example:
```{r create_wordbank_eng, eval=FALSE}
#eng_wb_data <- create_wb_data("English (American)")
```

Creating saved Wordbank data one step at a time (potentially making changes between steps), for example:
```{r create_wordbank_eng_steps, eval=FALSE}
#eng_wg <- create_inst_data("English (American)", "WG")
#eng_ws <- create_inst_data("English (American)", "WS")
#eng_wg_summary <- collapse_inst_data(eng_wg)
#eng_ws_summary <- collapse_inst_data(eng_ws)
#eng_comb_summary <- combine_form_data(list(eng_wg_summary, eng_ws_summary))
```


# Load predictors

## Ratings and phonemes

Merge in the by-concept predictors (babiness, concreteness, etc) to the unilemmas and the by word predictors (phonemes) to the words/definitions.

```{r merge_unilemmas}
uni_lemmas <- extract_uni_lemmas(wb_data)
```

```{r load_predictors}
babiness_map <- c(word = "word", babiness = "babyAVG")
babiness <- uni_lemmas |> map_predictor("babiness", babiness_map)

valence_map <- c(word = "Word", valence = "V.Mean.Sum", arousal = "A.Mean.Sum")
valence <- uni_lemmas |> map_predictor("valence", valence_map)

concreteness_map <- c(word = "Word", concreteness = "Conc.M")
concreteness <- uni_lemmas |> map_predictor("concreteness", concreteness_map)
```

## CHILDES

Loading cached CHILDES metrics for English:
```{r load_childes_eng}
#eng_metrics <- load_childes_metrics("English (American)", uni_lemmas)
```

Loading cached CHILDES data for multiple languages:
```{r load_childes_xling}
childes_metrics <- load_childes_metrics(target_langs, uni_lemmas) |>
  filter(!is.na(uni_lemma))
  
```

Creating saved CHILDES metrics for English, potentially changing which metrics are computed and/or arguments that are passed to `childesr` functions:
```{r specify_childes, eval=FALSE}
metric_funs <- list(compute_count, compute_mlu, compute_positions, compute_length_char, compute_length_phon,  compute_n_sfx_cat, compute_n_type, compute_prefix, compute_verb_frame)
corpus_args <- list(corpus = NULL, role = NULL, role_exclude = "Target_Child",
                    age = NULL, sex = NULL, part_of_speech = NULL, token = "*")
#ita_childes <- get_token_metrics("Italian", metric_funs, corpus_args)
#spa_childes <- get_token_metrics("Spanish (Mexican)", metric_funs, corpus_args)
#eng_childes <- get_token_metrics("English (American)", metric_funs, corpus_args)
#ita_unilemmas <- get_uni_lemma_metrics("Italian", build_uni_lemma_map(uni_lemmas))
#spa_unilemmas <- get_uni_lemma_metrics("Spanish (Mexican)", #build_uni_lemma_map(uni_lemmas))
#eng_unilemmas <- get_uni_lemma_metrics("English (American)", #build_uni_lemma_map(uni_lemmas))

```

Creating saved CHILDES data for many languages:
```{r create_childes_xling, eval=FALSE}
#walk(target_langs, get_token_metrics, metric_funs, corpus_args)
#walk(target_langs, load_childes_metrics, uni_lemmas)
```

Get phonology via eSpeak for tokens that didn't get it from CHILDES:
```{r}
#phonemes <- uni_lemmas |> map_phonemes()
#childes_metrics <- childes_metrics |>
#  left_join(phonemes, by = c("language", "uni_lemma")) |>
#  mutate(length_char = coalesce(length_char, num_chars),
#         length_phon = coalesce(length_phon, num_phons)) |>
#  select(-c(num_chars, num_phons))
```


# Prepare data for modeling

## Frequency transformations

By default, `transform_counts()` transforms any column that starts with "count" by smoothing (add 1), normalizing, and log transforming, then renaming every column "count_x" to "freq_x". `residualize_freqs()` residualizes all columns that starts with "freq_" from the column "freq".

```{r prepare_frequency}
childes_metrics_ <- childes_metrics |> transform_counts() |> residualize_freqs() |>
  residualize_morph() 
```

## Combining sources

Combine mapped predictors and CHILDES predictors:

```{r merge_all}
predictor_data_list <- list(babiness, concreteness, childes_metrics_)
predictor_data <- predictor_data_list |>
  reduce(partial(full_join, by = c("language", "uni_lemma"))) 

predictor_data$n_distinct_frame[is.na(predictor_data$n_distinct_frame)] <- 0
predictor_data$prefix[is.na(predictor_data$prefix)] <- 0
```

## Setting predictors

```{r set_predictors}
predictor_sources <- list(
  c("freq", "freq_solo", "freq_last", "freq_first"),
  c("concreteness","babiness"),
  c("length_char"), #"length_phon"
  c("n_type", "n_sfx", "n_category"),
  c("mlu")
)

#predictor_sources_short <- list(
#  c("freq", "freq_solo", "freq_last", "freq_first", "mlu"),
#  c("concreteness","babiness"),
#  c("length_char"),
#  c("n_type"),
#  c("length_char", "length_phon"),
#  c("n_type", "n_sfx", "n_category")
#)

predictors<-unlist(predictor_sources)
```


## Preparing data for regression

```{r prep_data}
ref_cat ="nouns"

predictor_data_lexcat <- prep_lexcat(predictor_data, uni_lemmas, ref_cat) ## Set lexical categories

prefix_verb_frame <- predictor_data_lexcat |>
  select(c(uni_lemma, lexical_category, prefix, n_distinct_frame))
 
predictor_data_imputed <- predictor_data_lexcat |> ## Impute
  do_full_imputation(predictor_sources, max_steps = 20) |>
  left_join(prefix_verb_frame)
  

predictor_data_scaled <- do_scaling(predictor_data_imputed, predictors) |>## Scale  
  mutate(length_char = ifelse (language %in% sinotibetan_languages, NA, length_char))

```
 
## Merge with AOAs

```{r merge_aoa}

aoa_predictor_data <- predictor_data_scaled |> left_join(aoas) |>## merge with AOAs
  filter(!(aoa>36))
 
```

## Run model 

```{r}

run_main_model<-function(lang, aoa_predictor_data, predictors, uni_lemmas, ref_cat){
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts

  m<-fit_models(predictors, aoa_predictor_data_lexcat) |> 
    mutate(language=lang)
  return(m)
}

#predictors_short<- c("freq", "freq_solo", "freq_last", "freq_first", "length_char", #"length_phon", "mlu", "babiness",  "freq_solo", "concreteness", "n_type", "n_sfx", #"n_category")


aoa_models <- map_df(target_langs, run_main_model, aoa_predictor_data, predictors, uni_lemmas, ref_cat)

ref_cat_v ="predicates"

predictors_v <-  c("freq", "freq_solo", "freq_last", "freq_first","concreteness","babiness","length_char","n_type", "n_sfx", "n_category","mlu", "n_distinct_frame")

aoa_models_v <- fit_models(predictors_v, aoa_predictor_data |>filter(lexical_category =="predicates"), lexcat_interactions = FALSE )


#aoa_models_fw <- map_df(target_langs[0:16], run_main_model, aoa_predictor_data, predictors, uni_lemmas, #"function_words")

```

## Model outputs

Coefficients:
```{r}
aoa_models |> select(language, measure, coefs) |> unnest(coefs)
```

Summary stats:
```{r}
aoa_models |> select(language, measure, stats) |> unnest(stats)
```

Variance inflation factors:
```{r}
aoa_models |> select(language, measure, vifs) |> unnest(vifs)
```


## Cross-validation

```{r cross_validate}

run_cross_validation_model<-function(lang, meas, aoa_predictor_data, predictors, uni_lemmas, ref_cat){
  aoa_predictor_data <- aoa_predictor_data|> select(-lexical_category)|>
    filter(language==lang, measure==meas)
  
  aoa_predictor_data_lexcat <- prep_lexcat(aoa_predictor_data, uni_lemmas |> filter(language==lang), ref_cat) ## Set again lexical category contrasts

loo_df <- aoa_predictor_data_lexcat |>
  group_nest(language, meas) |>
  mutate(language=lang,
         measure=meas,
    loo_models = ifelse(language %in% sinotibetan_languages, map(data, fit_cv_models, list(make_predictor_formula(predictors_short))), map(data, fit_cv_models, list(make_predictor_formula(predictors)))),
         loo_preds = map2(loo_models, data, get_cv_preds),
         cv_results = map(loo_preds, get_cv_results))  
    
return(loo_df)
}

# for simple cv result extraction
xval_und_model <- map_df(unique((aoa_predictor_data %>% filter(measure=="understands"))$language),run_cross_validation_model,"understands", aoa_predictor_data, predictors, uni_lemmas, ref_cat) 
xval_prod_model <- map_df(target_langs,run_cross_validation_model,"produces", aoa_predictor_data, predictors, uni_lemmas, ref_cat)

loo_df <- bind_rows(xval_und_model, xval_prod_model)
```




```{r cross_validate_results}

cv_results <- loo_df |>
  select(language, measure, cv_results) |>
  unnest(cv_results)

loo_preds <- loo_df |>
  select(language, measure, loo_preds, data)

cv_results_pos <- loo_preds |>
  unnest(loo_preds) |> 
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

cv_results_cat <- loo_preds |>
  unnest(c(loo_preds, data), names_repair = ~ make.names(., unique = TRUE)) |>
  group_by(language, measure, category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

cv_results_lex <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  group_by(language, measure, lexical_category) |>
  summarise(mean_abs_dev = mean(abs_dev), sd_abs_dev = sd(abs_dev))

eng_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "English (American)", measure == "produces") |>
  arrange(desc(abs_dev)) |>
  head(50)

man_t_across_lang_lex_desc <- loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == "Spanish (Mexican)", measure == "understands") |>
  arrange(desc(abs_dev)) |>
  head(50)

```


```{r coefs}
lexcats <- set_names(
  paste0("lexical_category", 1:3),
  rownames(contrasts(aoa_models$group_data[[1]]$lexical_category))[1:3]
)


aoa_coefs <- aoa_models |>
  select(language, measure, coefs) |>
  unnest(coefs) |>
  filter(term != "(Intercept)") |>
  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
         effect = if_else(str_detect(term, ":"), "interaction", "main"),
         lexical_category = if_else(effect == "interaction",
                                    str_extract(term, "lexical_category[0-9]"),
                                    as.character(NA)),
         lexical_category = fct_recode(lexical_category, !!!lexcats),
         term = if_else(effect == "interaction",
                        str_remove(term, ":?lexical_category[0-9]:?"),
                        term),
         term = fct_recode(term, !!!lexcats),
         term = factor(term, levels = c(predictors, names(lexcats))),
         language = factor(language, levels = target_langs))


#aoa_coefs <- aoa_models |>
#  select(language, measure, coefs) |>
#  unnest(coefs) |>
#  filter(term != "(Intercept)") |>
#  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
#         effect = if_else(str_detect(term, ":"), "interaction", "main"),
#         lexical_category = if_else(effect == "interaction",
#                                    str_extract(term, "lexical_category[0-9]"),
#                                    as.character(NA)),
#         lexical_category = fct_recode(lexical_category, !!!lexcats),
#         term = if_else(effect == "interaction",
#                        str_remove(term, ":?lexical_category[0-9]:?"),
#                        term),
#         term = fct_recode(term, !!!lexcats),
#         term = factor(term, levels = c(predictors_short, names(lexcats))),
#         language = factor(language, levels = target_langs))


#lexcats_fw <- set_names(
#  paste0("lexical_category", 1:3),
#  rownames(contrasts(aoa_models_fw$group_data[[1]]$lexical_category))[1:3]
#)

#aoa_coefs_fw <- aoa_models_fw |>
#  select(language, measure, coefs) |>
#  unnest(coefs) |>
#  filter(term != "(Intercept)") |>
#  mutate(signif = if_else(p.value < 0.05, TRUE, FALSE),
#         effect = if_else(str_detect(term, ":"), "interaction", "main"),
#         lexical_category = if_else(effect == "interaction",
#                                    str_extract(term, "lexical_category[0-9]"),
#                                    as.character(NA)),
#         lexical_category = fct_recode(lexical_category, !!!lexcats),
#         term = if_else(effect == "interaction",
#                        str_remove(term, ":?lexical_category[0-9]:?"),
#                        term),
#         term = fct_recode(term, !!!lexcats),
#         term = factor(term, levels = c(predictors, names(lexcats_fw))),
#         language = factor(language, levels = target_langs))



```


```{r save data}
saveRDS(predictor_data_lexcat, "data/plots/predictor_data_lexcat.rds")
saveRDS(aoa_predictor_data, "data/plots/aoa_predictor_data.rds" )

saveRDS(aoa_coefs, "data/plots/aoa_coefs.rds" )

saveRDS(cv_results, "data/plots/cv_results.rds" )
saveRDS(cv_results_pos, "data/plots/cv_results_pos.rds" )
saveRDS(cv_results_cat, "data/plots/cv_results_cat.rds" )
saveRDS(cv_results_lex, "data/plots/cv_results_lex.rds" )
saveRDS(eng_across_lang_lex_desc, "data/plots/eng_across_lang_lex_desc.rds" )


worst_predicted_unilemmas<- function(lang, meas){ loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(language == lang, measure == meas) |>
  arrange(desc(abs_dev)) |>
  head(50)
}

worst_uni_produce<-map_df(target_langs, worst_predicted_unilemmas, "produces") %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))

worst_uni_understand<-map_df(target_langs, worst_predicted_unilemmas, "understands") %>% 
  group_by(test_word) %>%
  summarise(count_lang=n(),
  lang_names = paste0(language, collapse = ","),
  aoa_mean=mean(aoa),
  aoa_pred_mean=mean(aoa_pred))


worst_uni_understand1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "understands") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))

worst_uni_produce1<-loo_preds |>
  unnest(cols = c(loo_preds))|>
  filter(measure == "produces") |>
  filter(abs_dev>5)|>
  arrange(desc(abs_dev))


saveRDS(worst_uni_produce, "data/plots/worst_uni_produce.rds" )
saveRDS(worst_uni_understand, "data/plots/worst_uni_understand.rds" )
saveRDS(worst_uni_produce1, "data/plots/worst_uni_produce1.rds" )
saveRDS(worst_uni_understand1, "data/plots/worst_uni_understand1.rds" )
```
