---
title: "childes_pipeline"
output: html_document
---

https://rpubs.com/gloukatou/739504

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(childesr)
library(data.table)
library(stringr)

```

```{r main function, message=FALSE, warning=FALSE}

word=""

get_childes_metrics <- function(args_, column="language", freq=FALSE, uttlength=FALSE, wordlen = FALSE ){
  data_<- do.call(get_data, args_)
  if (freq == TRUE){
  wordcount = frequency(data_$tokens, column)
  }
  if (uttlength == TRUE){
  mlu = mlu(data_$utterances, data_$tokens, column)
  }
  if (wordlen == TRUE){
  wlength = wordlength( data_$tokens)
  }
  args = as.character(args_["lang"])
  metrics <- full_join(wordcount, mlu) %>% full_join(wlength) %>% distinct()
  output<- metrics %>% mutate(args = as.character(args_["lang"]))
  return(output)
}  
```


```{r get data,  message=FALSE, warning=FALSE}

get_data<-function(lang = NULL,
                   corpus = NULL,
                   speaker_role = NULL, 
                   speaker_role_exclude = "Target_Child", 
                   child_age = NULL, 
                   child_sex = NULL, 
                   pos = NULL, 
                   word)
                   {
speakers<-get_speaker_statistics(corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)

utterances <-get_utterances(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)

tokens_ <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, token="*")

tokens1<-tokens_ %>% group_by(corpus_name) %>%  count()  %>% rename(corpuscount = n)
tokens2<-tokens_ %>% group_by(speaker_role) %>%  count()  %>% rename(speakercount = n)
tokens3<-tokens_ %>% group_by(target_child_name) %>%  count()  %>% rename(targetchildcount = n)
tokens_ <- tokens_ %>% left_join(tokens1)  %>% left_join(tokens2)  %>% left_join(tokens3)

if (word != ""){
tokens <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, part_of_speech = pos, token = word) %>%
  left_join(tokens_)}
else {tokens <- tokens_}

return(list(utterances = data.table(utterances), speakers =data.table(speakers), tokens=data.table(tokens))) #return a list of dataframes
}
```



```{r predictors,  message=FALSE, warning=FALSE}
#predictors:word_counts sent_lengths final_counts solo_counts concreteness frequency word_length valence babiness
#level on which user wants their predictor e.g. target_child_id, speaker_id, transcript_id

frequency<-function(tokens, column = "language"){ #target_child_id, speaker_id corpus_name
total<-nrow(tokens)  
tokens_fr <- tokens %>% 
  group_by(!!column, gloss)  %>% #corpuscount
    count()  %>% 
      rename(wordcount = n) %>% 
        mutate(freq= wordcount/total) %>%  #to fix with speakercount, targetchildcount, corpuscount
         mutate(logfreq= log(freq)) %>%
          ungroup() %>% 
            select(gloss, freq, logfreq)
return(tokens_fr)
} 
  
mlu <- function(utterances, tokens, column = "language"){
utterances_mlu<- utterances %>%
    mutate(utt_length = sapply(strsplit(utterances$gloss, " "), length)) %>%
     select(id, utt_length) %>%  
      rename(utterance_id = id)
tokens_mlu<- tokens %>% 
    left_join(utterances_mlu) %>%
     group_by(gloss)  %>% 
      summarise(mlu = mean(utt_length)) %>%  ###tofix column
        select(gloss, mlu)
return(tokens_mlu)
}

wordlength <- function(tokens){
tokens_length <- tokens %>%
   mutate(wordcount= str_count(gloss)) %>%
   distinct() %>%
   select(gloss, wordcount)
return(tokens_length)  
}

```


```{r example,  message=FALSE, warning=FALSE}

args_<-list(lang = "ita", word="")

output<- get_childes_metrics(args_, uttlength = TRUE, freq = TRUE, wordlen = TRUE)

names(output)[names(output) == "gloss"] <- "word"
output$args[output$args == "ita"] <- "Italian"

write.csv(output,"output_.csv" )  

```

```{r stemming,  message=FALSE, warning=FALSE}
#from mikabr/aoa_prediction

uni_lemmas <-read.csv("/saved_data/uni_lemmas.RData")


norm_lang <- function(lang)
  lang %>% tolower() %>% strsplit(" ") %>% map_chr(~.x[1])

source("stemmer.R")

transforms <- c(
  function(x) gsub("(.*) \\(.*\\)", "\\1", x),
  function(x) gsub(" ", "_", x),
  function(x) gsub(" ", "+", x),
  function(x) gsub("(.+) \\1", "\\1", x)
)

apply_transforms <- function(str) {
  transforms %>% map_chr(~.x(str))
}

special_case_files <- list.files("/predictors/childes/special_cases/")

special_case_map <-  map_df(special_case_files, function(case_file) {
  
  lang <- case_file %>% strsplit(".csv") %>% unlist()
  special_cases <- read_csv(file.path("/predictors/childes/special_cases/",
                                      case_file),
                            col_names = FALSE)
  
  map_df(1:nrow(special_cases), function(i) {
    uni_lemma <- special_cases$X1[i]
    options <- special_cases[i, 3:ncol(special_cases)] %>%
      as.character() %>%
      discard(is.na)
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique() #apply transforms to special cases
    data_frame(language = lang,
               uni_lemma = rep(uni_lemma, 2 * length(trans_opts)),
               stem = c(trans_opts, stem(trans_opts, lang)))
  })
  
})

pattern_map <- uni_lemmas %>%
  split(paste(.$language, .$uni_lemma, .$words)) %>%
  map_df(function(uni_data) {
    language <- uni_data$language %>% norm_lang()
    uni_lemma <- uni_data$uni_lemma
    options <- uni_data$words %>% strsplit(", ") %>% unlist() %>%
      strsplit("/") %>% unlist()
    options <- c(options, stem(options, language)) %>% unique() #stemming with Snowball
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    trans_opts <- c(trans_opts, stem(trans_opts, language)) %>% unique()
    data_frame(language = rep(uni_data$language, length(trans_opts)),
               uni_lemma = rep(uni_lemma, length(trans_opts)),
               stem = trans_opts)
  })

case_map <- bind_rows(special_case_map, pattern_map) %>% distinct()

```

```{r map_cases,  message=FALSE, warning=FALSE}

load_childes_data <- function(lang) {
  read_csv(sprintf("output_.csv",
                   norm_lang(lang))) %>%
    filter(!is.na(word)) %>%
    mutate(types = n(),
           tokens = sum(wordcount),
           stem = stem(word, norm_lang(lang)),
           language = lang) %>%
    full_join(case_map %>% filter(language == lang))
}

  #%>%
   # group_by(uni_lemma, types, tokens) %>%
    #summarise(MLU = weighted.mean(mean_sent_length, word_count, na.rm = TRUE),
     #         word_count = sum(word_count, na.rm = TRUE),
      #        MLU = ifelse(word_count < 10, NA, MLU),
       #       final_count = sum(final_count, na.rm = TRUE),
        #      solo_count = sum(solo_count, na.rm = TRUE),
         #     language = lang)

childes_data <- map_df(languages, load_childes_data)

#write.csv(childes_data,"childes_data.csv" )  


```

