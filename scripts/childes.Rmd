---
title: "childes_pipeline"
output: html_document
---

https://rpubs.com/gloukatou/739504

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(childesr)
library(data.table)
library(stringr)

###add word characters metric 
#### special cases for lemmatization
####exclude boccia/bocciata from mouth, barbie how to not consider a stem? remove "A"

```

```{r main function, message=FALSE, warning=FALSE}

word=""

get_childes_metrics <- function(args_, column="language", freq=FALSE, uttlength=FALSE, wordlen = FALSE, order = FALSE ){
  data_<- do.call(get_data, args_)
  if (freq == TRUE){
  wordcount = frequency(data_$tokens)
  }
  if (uttlength == TRUE){
  mlu = mlu(data_$utterances, data_$tokens, column)
  }
  if (wordlen == TRUE){
  wlength = wordlength( data_$tokens)
  }
  if (order == TRUE){
  worder = order(data_$utterances, data_$tokens, column)
  }
  args = as.character(args_["lang"])

  metrics <- full_join(wordcount, mlu) %>% full_join(wlength) %>% full_join(worder) %>% distinct()
  output<- metrics %>% mutate(args = as.character(args_["lang"]))
  return(output)
}  
```


```{r get data,  message=FALSE, warning=FALSE}

get_data<-function(lang = NULL,
                   corpus = NULL,
                   speaker_role = NULL, 
                   speaker_role_exclude = "Target_Child", 
                   child_age = NULL, 
                   child_sex = NULL, 
                   pos = NULL, 
                   word)
                   {
speakers<-get_speaker_statistics(corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)

utterances <-get_utterances(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)
####tolower utterances
tokens_ <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, token="*")

tokens1<-tokens_ %>% group_by(corpus_name) %>%  count()  %>% rename(corpuscount = n)
tokens2<-tokens_ %>% group_by(speaker_role) %>%  count()  %>% rename(speakercount = n)
tokens3<-tokens_ %>% group_by(target_child_name) %>%  count()  %>% rename(targetchildcount = n)
tokens_ <- tokens_ %>% left_join(tokens1)  %>% left_join(tokens2)  %>% left_join(tokens3)

if (word != ""){
tokens <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, part_of_speech = pos, token = word) %>%
  left_join(tokens_)}

else {tokens <- tokens_}

####tolower tokens
tokens$gloss <-tolower(tokens$gloss)
utterances$gloss <-tolower(utterances$gloss)

return(list(utterances = data.table(utterances), speakers =data.table(speakers), tokens=data.table(tokens))) #return a list of dataframes
}
```



```{r predictors,  message=FALSE, warning=FALSE}
#level on which user wants their predictor e.g. target_child_id, speaker_id, transcript_id

frequency<-function(tokens, column = "language"){ #target_child_id, speaker_id corpus_name
total<-nrow(tokens)  
tokens_fr <- tokens %>% 
  group_by(!!column, gloss)  %>% #corpuscount
    count()  %>% 
      rename(wordcount = n) %>% 
        mutate(freq= wordcount/total) %>%  #to fix with speakercount, targetchildcount, corpuscount
         mutate(logfreq= log(freq)) %>%
          ungroup() %>% 
            select(gloss, freq, logfreq)
return(tokens_fr)
} 
  
mlu <- function(utterances, tokens, column = "language"){
utterances_mlu<- utterances %>%
    mutate(utt_length = sapply(strsplit(utterances$gloss, " "), length)) %>%
     select(id, utt_length) %>%  
      rename(utterance_id = id)
tokens_mlu<- tokens %>% 
    left_join(utterances_mlu) %>%
     group_by(gloss)  %>% 
      summarise(mlu = mean(utt_length)) %>%  ###tofix column
        select(gloss, mlu)
return(tokens_mlu)
}

wordlength <- function(tokens){
tokens_length <- tokens %>%
   mutate(wordcount= str_count(gloss)) %>%
   distinct() %>%
   select(gloss, wordcount)
return(tokens_length)  
}


find_positions <- function(tokens_pos){
tokens_pos_ <- tokens_pos %>%
  mutate(utt_length = sapply(strsplit(tokens_pos$sentence, " "), length)) %>%
  mutate(lastword=word(sentence, -1)) %>%
  mutate(firstword=word(sentence, 1)) %>% 
  mutate(order = ifelse(utt_length == 1, "solo", ifelse(as.character(lastword) == as.character(gloss),"last",ifelse(as.character(firstword) == as.character(gloss), "first", "other")))) 
return(tokens_pos_)  
}

order <- function(utterances, tokens, column = "language"){
utterances_pos <- utterances %>% 
  select(id, gloss) %>%
    rename(sentence = gloss ) %>%
    rename(utterance_id = id)
tokens<- tokens %>%
  left_join(utterances_pos) %>%
    select(id, gloss, sentence)
tokens_pos <- find_positions(tokens)
tokens_count_ <- tokens_pos  %>% 
  count(gloss)
tokens_final <- tokens_pos  %>%
  count(gloss, order) %>%
    rename(ordergloss=n) %>% 
      left_join(tokens_count_) %>%
        mutate(solo = ifelse(as.character(order) == "solo",ordergloss,NA)) %>%
        mutate(last = ifelse(as.character(order) == "last",ordergloss, NA)) %>%
        mutate(first = ifelse(as.character(order) == "first",ordergloss, NA)) %>%
        mutate(other = ifelse(as.character(order) == "other",ordergloss, NA)) %>%
          select(gloss, solo, last, first, other, n)
return(tokens_final)
}

```


```{r example,  message=FALSE, warning=FALSE}

args_<-list(lang = "ita", word="")

output<- get_childes_metrics(args_, uttlength = TRUE, freq = TRUE, wordlen = TRUE, order = TRUE)

names(output)[names(output) == "gloss"] <- "word"
output$args[output$args == "ita"] <- "Italian"

write.csv(output,"childes_metrics.csv" )  

```

```{r stemming,  message=FALSE, warning=FALSE}
#adapted slightly from mikabr/aoa_prediction
library(SnowballC)

norm_lang <- function(lang)
  lang %>% tolower() %>% strsplit(" ") %>% map_chr(~.x[1])

source("stemmer.R")

transforms <- c(
  function(x) gsub("(.*) \\(.*\\)", "\\1", x),
  function(x) gsub(" ", "_", x),
  function(x) gsub(" ", "+", x),
  function(x) gsub("(.+) \\1", "\\1", x)
)

apply_transforms <- function(str) {
  transforms %>% map_chr(~.x(str))
}

special_case_files <- list.files("childes/special_cases/")

special_case_map <-  map_df(special_case_files, function(case_file) {
  
  lang <- case_file %>% strsplit(".csv") %>% unlist()
  special_cases <- read_csv(file.path("childes/special_cases/",
                                      case_file),
                            col_names = FALSE)
  
  map_df(1:nrow(special_cases), function(i) {
    uni_lemma <- special_cases$X1[i]
    options <- special_cases[i, 3:ncol(special_cases)] %>%
      as.character() %>%
      discard(is.na)
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique() #apply transforms to special cases
    data_frame(language = lang,
               uni_lemma = rep(uni_lemma, 2 * length(trans_opts)),
               stem = c(trans_opts, stem(trans_opts, lang)))
  })
  
}) 
#not forget to add () in croatian.py

loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    get(ls()[ls() != "fileName"])
}
uni_lemmas <- loadRData("_uni_lemmas.RData")

pattern_map <- uni_lemmas %>%
  split(paste(.$language, .$uni_lemma, .$words)) %>%
  map_df(function(uni_data) {
    language <- uni_data$language %>% norm_lang()
    uni_lemma <- uni_data$uni_lemma
    options <- uni_data$words %>% strsplit(", ") %>% unlist() %>%
      strsplit("/") %>% unlist()
    options <- c(options, stem(options, language)) %>% unique() #stemming with Snowball
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    trans_opts <- c(trans_opts, stem(trans_opts, language)) %>% unique()
    data_frame(language = rep(uni_data$language, length(trans_opts)),
               uni_lemma = rep(uni_lemma, length(trans_opts)),
               stem = trans_opts)
  })

case_map <- bind_rows(special_case_map, pattern_map) %>% distinct()

```

```{r map_cases,  message=FALSE, warning=FALSE}

lang = "Italian" #to fix

coalesce_by_column <- function(df) {
    return(coalesce(df[1], df[2], df[3], df[4]))
}

load_childes_data <- function(lang) {
  df <- read_csv(sprintf("childes_metrics.csv",
                   norm_lang(lang))) %>%
    filter(!is.na(word)) %>%
    mutate(types = n(),
           stem = stem(word, norm_lang(lang))) %>%
    full_join(case_map %>% filter(language == lang)) %>%
    group_by(uni_lemma, language) %>%
    filter(!is.na(uni_lemma)) %>%
    filter(!is.na(word)) %>%
    group_by(word) %>%
    summarise_all(coalesce_by_column) %>%
    group_by(uni_lemma) %>%
    summarise(nb_realisations=n(),
              words = list(unique(word)),
              word_count = sum(n, na.rm = TRUE),
              character_count = mean(wordcount, na.rm = TRUE),
              mean_freq=sum(freq, na.rm =TRUE),
              mean_mlu=mean(mlu, na.rm = TRUE),
              mean_solo=sum(solo, na.rm = TRUE)/word_count,
              mean_last=sum(last, na.rm = TRUE)/word_count,       #!!!!! for >1 words not 100% total
              mean_first=sum(first, na.rm = TRUE)/word_count,
              mean_other=sum(other, na.rm = TRUE)/word_count)}  
    
childes_data <- map_df(lang, load_childes_data)
childes_data$words <- vapply(childes_data$words, paste, collapse = ", ", character(1L))

write.csv(childes_data,"unilemma_metrics.csv" )  


```

