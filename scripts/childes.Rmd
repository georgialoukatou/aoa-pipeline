---
title: "childes_pipeline"
output: html_document
---

https://rpubs.com/gloukatou/739504

```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(childesr)
library(data.table)
library(stringr)

####TODOs: lower case tokens, remove "a", -solo in utterance
#### special cases for lemmatization
####exclude barbie

```

```{r main function, message=FALSE, warning=FALSE}

word=""

get_childes_metrics <- function(args_, column="language", freq=FALSE, uttlength=FALSE, wordlen = FALSE, order = FALSE ){
  data_<- do.call(get_data, args_)
  if (freq == TRUE){
  wordcount = frequency(data_$tokens, column)
  }
  if (uttlength == TRUE){
  mlu = mlu(data_$utterances, data_$tokens, column)
  }
  if (wordlen == TRUE){
  wlength = wordlength( data_$tokens)
  }
  if (order == TRUE){
  worder = order(data_$utterances, data_$tokens, column)
  }
  args = as.character(args_["lang"])
  metrics <- full_join(wordcount, mlu, worder) %>% full_join(wlength) %>% distinct()
  output<- metrics %>% mutate(args = as.character(args_["lang"]))
  return(output)
}  
```


```{r get data,  message=FALSE, warning=FALSE}

get_data<-function(lang = NULL,
                   corpus = NULL,
                   speaker_role = NULL, 
                   speaker_role_exclude = "Target_Child", 
                   child_age = NULL, 
                   child_sex = NULL, 
                   pos = NULL, 
                   word)
                   {
speakers<-get_speaker_statistics(corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)

utterances <-get_utterances(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex)
####tolower utterances
tokens_ <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, token="*")

tokens1<-tokens_ %>% group_by(corpus_name) %>%  count()  %>% rename(corpuscount = n)
tokens2<-tokens_ %>% group_by(speaker_role) %>%  count()  %>% rename(speakercount = n)
tokens3<-tokens_ %>% group_by(target_child_name) %>%  count()  %>% rename(targetchildcount = n)
tokens_ <- tokens_ %>% left_join(tokens1)  %>% left_join(tokens2)  %>% left_join(tokens3)

if (word != ""){
tokens <-get_tokens(language = lang, corpus = corpus, role = speaker_role, role_exclude = speaker_role_exclude, age = child_age, sex = child_sex, part_of_speech = pos, token = word) %>%
  left_join(tokens_)}

else {tokens <- tokens_}

####tolower tokens

return(list(utterances = data.table(utterances), speakers =data.table(speakers), tokens=data.table(tokens))) #return a list of dataframes
}
```



```{r predictors,  message=FALSE, warning=FALSE}
#predictors:word_counts sent_lengths final_counts solo_counts concreteness frequency word_length valence babiness
#level on which user wants their predictor e.g. target_child_id, speaker_id, transcript_id

frequency<-function(tokens, column = "language"){ #target_child_id, speaker_id corpus_name
total<-nrow(tokens)  
tokens_fr <- tokens %>% 
  group_by(!!column, gloss)  %>% #corpuscount
    count()  %>% 
      rename(wordcount = n) %>% 
        mutate(freq= wordcount/total) %>%  #to fix with speakercount, targetchildcount, corpuscount
         mutate(logfreq= log(freq)) %>%
          ungroup() %>% 
            select(gloss, freq, logfreq)
return(tokens_fr)
} 
  
mlu <- function(utterances, tokens, column = "language"){
utterances_mlu<- utterances %>%
    mutate(utt_length = sapply(strsplit(utterances$gloss, " "), length)) %>%
     select(id, utt_length) %>%  
      rename(utterance_id = id)
tokens_mlu<- tokens %>% 
    left_join(utterances_mlu) %>%
     group_by(gloss)  %>% 
      summarise(mlu = mean(utt_length)) %>%  ###tofix column
        select(gloss, mlu)
return(tokens_mlu)
}

wordlength <- function(tokens){
tokens_length <- tokens %>%
   mutate(wordcount= str_count(gloss)) %>%
   distinct() %>%
   select(gloss, wordcount)
return(tokens_length)  
}


order <- function(utterances, tokens, column = "language"){
utterances_pos <- utterances %>% 
  select(id, gloss) %>%
  rename(sentence = gloss ) %>%
  rename(utterance_id = id)
tokens_pos <- tokens %>%
  left_join(utterances_pos) %>%
  select(id, gloss, sentence)
tokens_pos_ <- tokens_pos %>%
   mutate(lastword=word(sentence, -1)) %>%
   mutate(firstword=word(sentence, 1)) %>% 
   mutate(order = ifelse(as.character(lastword) == as.character(gloss),"last",ifelse(as.character(firstword) == as.character(gloss), "first", "other")))  #####fix when word repeats in utterance %>%
tokens_count_ <- tokens_pos_  %>%
  count(gloss)
tokens_final <- tokens_pos_  %>%
  count(gloss, order) %>%
  rename(ordergloss=n) %>% 
  left_join(tokens_count_) %>%
  mutate(last = ifelse(as.character(order) == "last",ordergloss/n, "NA")) %>%
  mutate(first = ifelse(as.character(order) == "first",ordergloss/n, "NA")) %>%
  mutate(other = ifelse(as.character(order) == "other",ordergloss/n, "NA")) %>%
  select(gloss, last, first, other)
return(tokens_final)
}

```


```{r example,  message=FALSE, warning=FALSE}

args_<-list(lang = "ita", word="")

output<- get_childes_metrics(args_, uttlength = TRUE, freq = TRUE, wordlen = TRUE, order = TRUE)

names(output)[names(output) == "gloss"] <- "word"
output$args[output$args == "ita"] <- "Italian"

#write.csv(output,"/Users/lscpuser/Documents/aoa-pipeline/scripts/output_.csv" )  
write.csv(output,"/Users/lscpuser/Documents/output_1.csv" )  

```

```{r stemming,  message=FALSE, warning=FALSE}
#adapted slightly from mikabr/aoa_prediction
library(SnowballC)

norm_lang <- function(lang)
  lang %>% tolower() %>% strsplit(" ") %>% map_chr(~.x[1])

source("stemmer.R")

transforms <- c(
  function(x) gsub("(.*) \\(.*\\)", "\\1", x),
  function(x) gsub(" ", "_", x),
  function(x) gsub(" ", "+", x),
  function(x) gsub("(.+) \\1", "\\1", x)
)

apply_transforms <- function(str) {
  transforms %>% map_chr(~.x(str))
}

special_case_files <- list.files("/Users/lscpuser/Documents/aoa-prediction/aoa_unified/aoa_loading/predictors/childes/special_cases/")

special_case_map <-  map_df(special_case_files, function(case_file) {
  
  lang <- case_file %>% strsplit(".csv") %>% unlist()
  special_cases <- read_csv(file.path("/Users/lscpuser/Documents/aoa-prediction/aoa_unified/aoa_loading/predictors/childes/special_cases/",
                                      case_file),
                            col_names = FALSE)
  
  map_df(1:nrow(special_cases), function(i) {
    uni_lemma <- special_cases$X1[i]
    options <- special_cases[i, 3:ncol(special_cases)] %>%
      as.character() %>%
      discard(is.na)
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique() #apply transforms to special cases
    data_frame(language = lang,
               uni_lemma = rep(uni_lemma, 2 * length(trans_opts)),
               stem = c(trans_opts, stem(trans_opts, lang)))
  })
  
}) 
#not forget to add () in croatian.py

loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    get(ls()[ls() != "fileName"])
}
uni_lemmas <- loadRData("_uni_lemmas.RData")

pattern_map <- uni_lemmas %>%
  split(paste(.$language, .$uni_lemma, .$words)) %>%
  map_df(function(uni_data) {
    language <- uni_data$language %>% norm_lang()
    uni_lemma <- uni_data$uni_lemma
    options <- uni_data$words %>% strsplit(", ") %>% unlist() %>%
      strsplit("/") %>% unlist()
    options <- c(options, stem(options, language)) %>% unique() #stemming with Snowball
    trans_opts <- map(options, apply_transforms) %>% unlist() %>% unique()
    trans_opts <- c(trans_opts, stem(trans_opts, language)) %>% unique()
    data_frame(language = rep(uni_data$language, length(trans_opts)),
               uni_lemma = rep(uni_lemma, length(trans_opts)),
               stem = trans_opts)
  })

case_map <- bind_rows(special_case_map, pattern_map) %>% distinct()

```

```{r map_cases,  message=FALSE, warning=FALSE}

lang = "Italian" #to fix

load_childes_data <- function(lang) {
  read_csv(sprintf("/Users/lscpuser/Documents/aoa-pipeline/scripts/output_.csv",
                   norm_lang(lang))) %>%
    filter(!is.na(word)) %>%
    mutate(types = n(),
           tokens = sum(wordcount),
           stem = stem(word, norm_lang(lang))) %>%
    full_join(case_map %>% filter(language == lang)) %>%
    group_by(uni_lemma, language) %>%
    filter(!is.na(uni_lemma)) %>%
    filter(!is.na(word)) %>%
    summarise(NB_REALISATIONS=n(),
              FREQ=sum(freq, na.rm =TRUE),
              MLU=weighted.mean(mlu, FREQ, na.rm = TRUE),
              LAST=weighted.mean(last, FREQ, na.rm = TRUE),
              FIRST=weighted.mean(first, FREQ, na.rm = TRUE),
              OTHER=weighted.mean(other, FREQ, na.rm = TRUE) )} 
              
  #  summarise(MLU = weighted.mean(mean_sent_length, word_count, na.rm = TRUE),
     #         word_count = sum(word_count, na.rm = TRUE),
      #        MLU = ifelse(word_count < 10, NA, MLU),
       #       final_count = sum(final_count, na.rm = TRUE),
        #      solo_count = sum(solo_count, na.rm = TRUE),
         #     language = lang)

childes_data <- map_df(lang, load_childes_data)


write.csv(childes_data,"/Users/lscpuser/Documents/childes_data_1.csv" )  


```

